{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de783656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1982d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/Vidhya/Desktop/Study/Semester 1/ML/Assignments/weatherAUS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f355fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop unnecessary columns\n",
    "# #data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# # Handle missing values\n",
    "# data.dropna(inplace=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'],format='%d-%m-%Y')\n",
    "data['Date'] = data['Date'].values.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73b5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numerical using label encoding\n",
    "categorical_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d23f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X = data.drop('RainTomorrow', axis=1)  # Features\n",
    "y = data['RainTomorrow']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2154dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "# Extract year, month, and day from the 'Date' column\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "data['Day'] = pd.to_datetime(data['Date']).dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001c296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the difference between max and min temperature\n",
    "# data['TempDiff'] = data['MaxTemp'] - data['MinTemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc905d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform imputation for missing values in features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef6d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Without Data Pre-processing***********\n",
      "Logistic Regression Accuracy on Test Set without Pre-processing: 76.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vidhya\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train Model without through pre-processing\n",
    "# Initialize Logistic Regression function\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"******Without Data Pre-processing***********\")\n",
    "print(\"Logistic Regression Accuracy on Test Set without Pre-processing: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb77344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaustav\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/Vidhya/Desktop/Study/Semester 1/ML/Assignments/weatherAUS.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Dropping columns with more than 40% missing data\n",
    "df_dropped = df.drop(columns=['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'])\n",
    "\n",
    "# Imputing missing values for numerical columns using median\n",
    "numerical_columns = df_dropped.select_dtypes(include=['float64']).columns\n",
    "df_dropped[numerical_columns] = df_dropped[numerical_columns].fillna(df_dropped[numerical_columns].median())\n",
    "\n",
    "# Imputing missing values for categorical columns using mode\n",
    "categorical_columns = df_dropped.select_dtypes(include=['object']).columns\n",
    "df_dropped[categorical_columns] = df_dropped[categorical_columns].fillna(df_dropped[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Handling skewness by applying log transformation to highly skewed columns\n",
    "df_dropped['Rainfall'] = np.log1p(df_dropped['Rainfall'])  # log1p is used to avoid log(0)\n",
    "df_dropped['WindGustSpeed'] = np.log1p(df_dropped['WindGustSpeed'])\n",
    "\n",
    "# Encoding categorical variables using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_dropped, drop_first=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "if 'RainTomorrow_Yes' in df_encoded.columns:\n",
    "    X = df_encoded.drop('RainTomorrow_Yes', axis=1)\n",
    "    y = df_encoded['RainTomorrow_Yes']\n",
    "else:\n",
    "    X = df_encoded.drop('RainTomorrow', axis=1)\n",
    "    y = df_encoded['RainTomorrow']\n",
    "\n",
    "# Standardization\n",
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Feature Importance using Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_scaled.columns\n",
    "\n",
    "# Create a dataframe for feature importances\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Selecting top 20 features\n",
    "top_features = feature_importances.head(20)['Feature'].tolist()\n",
    "X_selected = X_scaled[top_features]\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Combining the cleaned and selected features with the target variable for final output\n",
    "# final_df_train = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
    "# final_df_test = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# # Saving the cleaned datasets to CSV files\n",
    "# train_file_path = 'cleaned_weather_train.csv'\n",
    "# test_file_path = 'cleaned_weather_test.csv'\n",
    "\n",
    "# final_df_train.to_csv(train_file_path, index=False)\n",
    "# final_df_test.to_csv(test_file_path, index=False)\n",
    "\n",
    "# print(f'Train file saved to {train_file_path}')\n",
    "# print(f'Test file saved to {test_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4ad2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on Test Set after Preprocessing: 84.33%\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model\n",
    "# Initialize Logistic Regression function\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy on Test Set after Preprocessing: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49ca8927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******After Hyperparameters Tuning***********\n",
      "Best parameters for Logistic Regression: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score for Logistic Regression: 84.80%\n",
      "Logistic Regression Accuracy on Test Set after tuning: 84.32%\n"
     ]
    }
   ],
   "source": [
    "print(\"******After Hyperparameters Tuning***********\")\n",
    "# Define the hyperparameter grid\n",
    "param_grid_log_reg = {'C': [0.01, 0.1, 1, 10, 100], # Inverse of regularization strength\n",
    "                      'penalty': ['l2'],   # Type of regularization\n",
    "                      'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],  # Optimization algorithm\n",
    "                      'max_iter': [100, 200, 300]                      # Maximum number of iterations\n",
    "                     }\n",
    "    \n",
    "# Set up GridSearchCV\n",
    "grid_search_log_reg = GridSearchCV(estimator=model, param_grid=param_grid_log_reg, cv=5, scoring='accuracy',error_score='raise')\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search_log_reg.fit(X_train, y_train)    \n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters for Logistic Regression:\", grid_search_log_reg.best_params_)\n",
    "print(\"Best score for Logistic Regression: {:.2f}%\".format(grid_search_log_reg.best_score_ *100))\n",
    "\n",
    "# Evaluate on test set\n",
    "best_log_reg = grid_search_log_reg.best_estimator_\n",
    "y_pred_log_reg = best_log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy on Test Set after tuning: {:.2f}%\".format(accuracy_score(y_test, y_pred_log_reg)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e155fa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** After Hyperparameters identification - Retraining Model***********\n",
      "Logistic Regression Accuracy on Test Set after : 84.32%\n",
      "Confusion Matrix:\n",
      "[[4475  251]\n",
      " [ 690  584]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.95      0.90      4726\n",
      "        True       0.70      0.46      0.55      1274\n",
      "\n",
      "    accuracy                           0.84      6000\n",
      "   macro avg       0.78      0.70      0.73      6000\n",
      "weighted avg       0.83      0.84      0.83      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#********* After tuning\n",
    "model = LogisticRegression(C= 0.1, max_iter = 100, penalty =  'l2', solver = 'liblinear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"****** After Hyperparameters identification - Retraining Model***********\")\n",
    "print(\"Logistic Regression Accuracy on Test Set after Tuning : {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "078b194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on Test Set: 77.47%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "model_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_tree.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_tree = accuracy_score(y_test, y_pred)\n",
    "conf_matrix_tree = confusion_matrix(y_test, y_pred)\n",
    "class_report_tree = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Decision Tree Accuracy on Test Set: {:.2f}%\".format(accuracy_tree * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bddfca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "Best score for Decision Tree: 83.65%\n",
      "Decision Tree Accuracy on Test Set: 82.07%\n",
      "Confusion Matrix:\n",
      "[[4023  703]\n",
      " [ 649  625]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.85      0.86      4726\n",
      "        True       0.47      0.49      0.48      1274\n",
      "\n",
      "    accuracy                           0.77      6000\n",
      "   macro avg       0.67      0.67      0.67      6000\n",
      "weighted avg       0.78      0.77      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"******After Hyperparameters Tuning***********\")\n",
    "# Define the hyperparameter grid\n",
    "param_grid = { 'criterion': ['gini', 'entropy'],            # Function to measure the quality of a split\n",
    "               'max_depth': [10, 20, 30, 40],         # Maximum depth of the tree\n",
    "               'min_samples_split': [2, 5, 10],              # Minimum number of samples required to split an internal node\n",
    "               'min_samples_leaf': [1, 2, 5, 10],            # Minimum number of samples required at a leaf node\n",
    "               'max_features': ['sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_decision_tree = GridSearchCV(estimator=model_tree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters for Decision Tree:\", grid_search_decision_tree.best_params_)\n",
    "print(\"Best score for Decision Tree: {:.2f}%\".format(grid_search_decision_tree.best_score_*100))\n",
    "    \n",
    "\n",
    "# Evaluate on test set\n",
    "best_decision_tree = grid_search_decision_tree.best_estimator_\n",
    "y_pred_decision_tree = best_decision_tree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy on Test Set: {:.2f}%\".format(accuracy_score(y_test, y_pred_decision_tree)*100))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_tree)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Tree\n",
    "# plt.figure(figsize=(12,8))\n",
    "# tree.plot_tree(model_tree, feature_names=None, class_names=None, filled=True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
